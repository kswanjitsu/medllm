accelerate launch finetune.py \
 --model "decapoda-research/llama-13b-hf-int4" \
 --val_set_size = 0.1 \
 --prompt_template = "prompts/medalpaca.json" \
 --model_max_length = 2048 \
 --train_on_inputs = True \
 --load_from_s3 = True \
 --aws_access_key_id = "AKIATJDARGJE6RBSRJUF" \
 --aws_secret_access_key = "AR6usGEl4vKq/RGDTIjTAMqXuD22LIqhHDPtoZJH/" \
 --s3_bucket = "sagemaker-studio-y9v8f5akyup" \
 --s3_filename = "final_instruction_examples.jsonl" \
 --train_in_8bit = True \
 --use_lora = True \
 --lora_r = 8 \
 --lora_alpha = 16 \
 --lora_dropout = 0.1 \
 --lora_target_modules = '[q_proj,v_proj]' \
per_device_batch_size: int = 2,
num_epochs: int = 3,
learning_rate: float = 2e-5,
global_batch_size: int = 128,
output_dir: str = "./output",
save_total_limit: int = 3,
eval_steps: int = 200,
device_map: str = "auto",
group_by_length: bool = False,
wandb_run_name: str = "test",
use_wandb: bool = False,
wandb_project: str = "medalpaca",
optim: str = "adamw_torch",
lr_scheduler_type: str = "cosine",
fp16: bool = True,
bf16: bool = False,
gradient_checkpointing: bool = False,
warmup_steps: int = 100,
fsdp: str = "full_shard auto_wrap",
fsdp_transformer_layer_cls_to_wrap: str = "LlamaDecoderLayer",
**kwargs